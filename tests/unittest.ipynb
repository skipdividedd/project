{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neurox\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bb4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import unittest\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad96a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "from data_classes import get_file_names, ConvertSample, GetEmbeddings\n",
    "from probing_classes import get_converted_filenames, Experiment, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef822400",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'{str(Path(os.getcwd()).parents[0])}/data/data_en_ewt'\n",
    "file1 = data_path+'/en_ewt_Case.csv'\n",
    "file2 = data_path+'/en_ewt_Definite.csv'\n",
    "large_data_path = f'{str(Path(os.getcwd()).parents[0])}/data/large_data_en_ewt'\n",
    "\n",
    "class TestConverter(unittest.TestCase): #tests ConvertSample\n",
    "    \n",
    "    def test_get_file_names(self):\n",
    "        # asserts we have 2 files in data_path (.csv files)\n",
    "        a = get_file_names(data_path)\n",
    "        b = 2\n",
    "        self.assertEqual(len(a), b)\n",
    "        \n",
    "    def setUp(self):\n",
    "        # creates splitter -- train, test, control task\n",
    "        self.splitter = ConvertSample(file1, train_size=3, test_size=3)\n",
    "        self.splitter_ = ConvertSample(file2, train_size=3, test_size=3)\n",
    "        \n",
    "    def test_read(self):\n",
    "        # asserts read func works as supposed\n",
    "        a = len(self.splitter.read())\n",
    "        b = 9226\n",
    "        self.assertEqual(a, b)\n",
    "        \n",
    "    def test_sampler(self):\n",
    "        # asserts we have same labels in train and test\n",
    "        train, test = self.splitter.stupid_sampler()\n",
    "        self.assertEqual(set(train.values()), set(test.values()))\n",
    "        \n",
    "    def test_permute(self):\n",
    "        # asserts train and control task are different dicts\n",
    "        train, test = self.splitter.stupid_sampler()    \n",
    "        b = self.splitter.using_shuffle(train)\n",
    "        self.assertEqual(train.keys(), b.keys())\n",
    "    \n",
    "    def test_create_dicts(self):\n",
    "        # asserts train and control task are different dicts\n",
    "        dict_train, dict_test, dict_task = self.splitter.create_dicts()\n",
    "        self.assertNotEqual(dict_train, dict_task)\n",
    "                         \n",
    "    def test_create_paths(self):\n",
    "        # asserts we actually create paths\n",
    "        self.assertIsNotNone(self.splitter.create_paths())\n",
    "    \n",
    "    def test_writter(self):\n",
    "        # asserts we get paths\n",
    "        self.assertIsNotNone(self.splitter.writer())\n",
    "        self.assertIsNotNone(self.splitter_.writer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d574fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_create_dicts (__main__.TestConverter) ... ok\n",
      "test_create_paths (__main__.TestConverter) ... ok\n",
      "test_get_file_names (__main__.TestConverter) ... ok\n",
      "test_permute (__main__.TestConverter) ... ok\n",
      "test_read (__main__.TestConverter) ... ok\n",
      "test_sampler (__main__.TestConverter) ... ok\n",
      "test_writter (__main__.TestConverter) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.248s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f921252e520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81a2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings\n",
    "path_to_file = large_data_path+'/data_Case'\n",
    "case = sorted(get_file_names(path_to_file))[2:]\n",
    "emb_case = GetEmbeddings(case[1], case[0])\n",
    "emb_case.jsons('bert-base-uncased')\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a9afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestError(unittest.TestCase):\n",
    "    def test1(self):\n",
    "        # asserts that func raises Error\n",
    "        with self.assertRaises(IndexError):\n",
    "            get_converted_filenames(large_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125f850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_create_dicts (__main__.TestConverter) ... ok\n",
      "test_create_paths (__main__.TestConverter) ... ok\n",
      "test_get_file_names (__main__.TestConverter) ... ok\n",
      "test_permute (__main__.TestConverter) ... ok\n",
      "test_read (__main__.TestConverter) ... ok\n",
      "test_sampler (__main__.TestConverter) ... ok\n",
      "test_writter (__main__.TestConverter) ... ok\n",
      "test1 (__main__.TestError) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.150s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f9212539130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f76fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings\n",
    "path_to_file = large_data_path+'/data_Definite'\n",
    "defin = sorted(get_file_names(path_to_file))[2:]\n",
    "emb_defin = GetEmbeddings(defin[1], defin[0])\n",
    "emb_defin.jsons('bert-base-uncased')\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb122c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true, control = get_converted_filenames(large_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522435db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestExperiment(unittest.TestCase): # tests Experiment\n",
    "    \n",
    "    def test_get_converted_filenames(self):\n",
    "        # asserts large_data_path has 2 directories\n",
    "        a = get_converted_filenames(large_data_path)\n",
    "        b = 2\n",
    "        self.assertEqual(len(a), b)\n",
    "        \n",
    "    def setUp(self):\n",
    "        # creates Experiment object\n",
    "        self.exp = Experiment(*true[0])\n",
    "        \n",
    "    def test_Exp_attributes(self):\n",
    "        # checks if attributes are extractable\n",
    "        self.assertEqual(self.exp.dataset, 'en_ewt')\n",
    "        self.assertEqual(self.exp.category, 'Case')\n",
    "        \n",
    "    def test_data_size(self):\n",
    "        # asserts we return a tuple of size 3\n",
    "        self.assertEqual(len(self.exp.data_size()), 3)\n",
    "        \n",
    "        \n",
    "class TestTrainer(unittest.TestCase): # tests Trainer\n",
    "    \n",
    "    def setUp(self):\n",
    "        # creates Trainer object for one and multiple categories\n",
    "        self.train_one = Trainer(true[0])\n",
    "        self.train_two = Trainer(true)\n",
    "    \n",
    "    def test_type_str(self):\n",
    "        # asserts differentiates type\n",
    "        self.assertTrue(self.train_one.type is str)\n",
    "        \n",
    "    def test_type_list(self):\n",
    "        self.assertTrue(self.train_two.type is list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb76afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_create_dicts (__main__.TestConverter) ... ok\n",
      "test_create_paths (__main__.TestConverter) ... ok\n",
      "test_get_file_names (__main__.TestConverter) ... ok\n",
      "test_permute (__main__.TestConverter) ... ok\n",
      "test_read (__main__.TestConverter) ... ok\n",
      "test_sampler (__main__.TestConverter) ... ok\n",
      "test_writter (__main__.TestConverter) ... ok\n",
      "test1 (__main__.TestError) ... FAIL\n",
      "test_Exp_attributes (__main__.TestExperiment) ... ok\n",
      "test_data_size (__main__.TestExperiment) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json activations from /home/senya/Документы/project/data/large_data_en_ewt/data_Case/activations_train.json...\n",
      "3 13.0\n",
      "Loading json activations from /home/senya/Документы/project/data/large_data_en_ewt/data_Case/activations_te.json...\n",
      "3 13.0\n",
      "Number of tokens:  3\n",
      "length of source dictionary:  59\n",
      "length of target dictionary:  3\n",
      "3\n",
      "Total instances: 3\n",
      "['To', 'Thanks', 'I']\n",
      "Number of samples:  3\n",
      "Stats: Labels with their frequencies in the final set\n",
      "Gen 1\n",
      "Nom 1\n",
      "Acc 1\n",
      "Number of tokens:  3\n",
      "length of source dictionary:  30\n",
      "length of target dictionary:  3\n",
      "3\n",
      "Total instances: 3\n",
      "['Please', 'Sounds', 'We']\n",
      "Number of samples:  3\n",
      "Stats: Labels with their frequencies in the final set\n",
      "Gen 1\n",
      "Nom 1\n",
      "Acc 1\n",
      "Loading json activations from /home/senya/Документы/project/data/large_data_en_ewt/data_Case/activations_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_get_converted_filenames (__main__.TestExperiment) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 13.0\n",
      "Loading json activations from /home/senya/Документы/project/data/large_data_en_ewt/data_Case/activations_te.json...\n",
      "3 13.0\n",
      "Number of tokens:  3\n",
      "length of source dictionary:  59\n",
      "length of target dictionary:  3\n",
      "3\n",
      "Total instances: 3\n",
      "['To', 'Thanks', 'I']\n",
      "Number of samples:  3\n",
      "Stats: Labels with their frequencies in the final set\n",
      "Gen 1\n",
      "Nom 1\n",
      "Acc 1\n",
      "Number of tokens:  3\n",
      "length of source dictionary:  30\n",
      "length of target dictionary:  3\n",
      "3\n",
      "Total instances: 3\n",
      "['Please', 'Sounds', 'We']\n",
      "Number of samples:  3\n",
      "Stats: Labels with their frequencies in the final set\n",
      "Gen 1\n",
      "Nom 1\n",
      "Acc 1\n",
      "Loading json activations from /home/senya/Документы/project/data/large_data_en_ewt/data_Case/activations_train.json...\n",
      "3 13.0\n",
      "Loading json activations from /home/senya/Документы/project/data/large_data_en_ewt/data_Case/activations_te.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_type_list (__main__.TestTrainer) ... ok\n",
      "test_type_str (__main__.TestTrainer) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test1 (__main__.TestError)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_191318/75263631.py\", line 5, in test1\n",
      "    get_converted_filenames(large_data_path)\n",
      "AssertionError: IndexError not raised\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 0.751s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 13.0\n",
      "Number of tokens:  3\n",
      "length of source dictionary:  59\n",
      "length of target dictionary:  3\n",
      "3\n",
      "Total instances: 3\n",
      "['To', 'Thanks', 'I']\n",
      "Number of samples:  3\n",
      "Stats: Labels with their frequencies in the final set\n",
      "Gen 1\n",
      "Nom 1\n",
      "Acc 1\n",
      "Number of tokens:  3\n",
      "length of source dictionary:  30\n",
      "length of target dictionary:  3\n",
      "3\n",
      "Total instances: 3\n",
      "['Please', 'Sounds', 'We']\n",
      "Number of samples:  3\n",
      "Stats: Labels with their frequencies in the final set\n",
      "Gen 1\n",
      "Nom 1\n",
      "Acc 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f92124572b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
